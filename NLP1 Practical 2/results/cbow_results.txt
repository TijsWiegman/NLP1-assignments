CBOW(
  (embed): Embedding(18280, 5)
  (linear): Linear(in_features=5, out_features=5, bias=True)
)
Iter 1000: loss=3874.5706, time=0.82s
iter 1000: dev acc=0.2289
new highscore
Iter 2000: loss=2634.7298, time=1.89s
iter 2000: dev acc=0.2252
Iter 3000: loss=2246.5680, time=2.97s
iter 3000: dev acc=0.2189
Iter 4000: loss=1908.0238, time=4.03s
iter 4000: dev acc=0.2307
new highscore
Iter 5000: loss=1794.1592, time=5.10s
iter 5000: dev acc=0.2543
new highscore
Iter 6000: loss=1761.1405, time=6.16s
iter 6000: dev acc=0.2489
Iter 7000: loss=1654.2571, time=7.21s
iter 7000: dev acc=0.2225
Iter 8000: loss=1636.8230, time=8.26s
iter 8000: dev acc=0.2570
new highscore
Iter 9000: loss=1597.6247, time=9.30s
iter 9000: dev acc=0.2670
new highscore
Iter 10000: loss=1574.0135, time=10.34s
iter 10000: dev acc=0.2589
Iter 11000: loss=1590.4586, time=11.37s
iter 11000: dev acc=0.2607
Iter 12000: loss=1585.6354, time=12.40s
iter 12000: dev acc=0.2570
Iter 13000: loss=1570.6653, time=13.43s
iter 13000: dev acc=0.2407
Iter 14000: loss=1580.9687, time=14.47s
iter 14000: dev acc=0.2171
Iter 15000: loss=1584.5907, time=15.50s
iter 15000: dev acc=0.2688
new highscore
Iter 16000: loss=1567.0525, time=16.54s
iter 16000: dev acc=0.2616
Iter 17000: loss=1580.9202, time=17.58s
iter 17000: dev acc=0.2434
Iter 18000: loss=1573.5752, time=18.61s
iter 18000: dev acc=0.2761
new highscore
Iter 19000: loss=1562.9973, time=19.65s
iter 19000: dev acc=0.2598
Iter 20000: loss=1555.2782, time=20.68s
iter 20000: dev acc=0.2643
Iter 21000: loss=1536.0454, time=21.72s
iter 21000: dev acc=0.2552
Iter 22000: loss=1559.8139, time=22.75s
iter 22000: dev acc=0.2543
Iter 23000: loss=1566.1201, time=23.79s
iter 23000: dev acc=0.2716
Iter 24000: loss=1552.3643, time=24.82s
iter 24000: dev acc=0.2661
Iter 25000: loss=1558.8950, time=25.86s
iter 25000: dev acc=0.2579
Iter 26000: loss=1540.7965, time=26.90s
iter 26000: dev acc=0.2797
new highscore
Iter 27000: loss=1522.8094, time=27.94s
iter 27000: dev acc=0.2743
Iter 28000: loss=1515.8847, time=28.97s
iter 28000: dev acc=0.2752
Iter 29000: loss=1546.0578, time=30.00s
iter 29000: dev acc=0.2816
new highscore
Iter 30000: loss=1534.7023, time=31.04s
iter 30000: dev acc=0.2870
new highscore
Done training
Loading best model
best model iter 30000: train acc=0.3220, dev acc=0.2870, test acc=0.2367
losses:
[3874.5706053844624, 2634.7297924207523, 2246.568041279912, 1908.0238127112389, 1794.1592421531677, 1761.1405493319035, 1654.257050871849, 1636.823014497757, 1597.6247476935387, 1574.0135385394096, 1590.458598613739, 1585.6353524327278, 1570.665280342102, 1580.968672811985, 1584.590710580349, 1567.0525150299072, 1580.920172572136, 1573.5751913189888, 1562.9972587823868, 1555.2782416939735, 1536.045381784439, 1559.8138607144356, 1566.1200672388077, 1552.3643290996552, 1558.8949915766716, 1540.796501338482, 1522.8093765974045, 1515.8846518993378, 1546.057817041874, 1534.7023113369942]
accuracies:
[0.22888283378746593, 0.22524977293369663, 0.21889191643960038, 0.23069936421435058, 0.254314259763851, 0.2488646684831971, 0.22252497729336967, 0.25703905540417804, 0.2670299727520436, 0.25885558583106266, 0.26067211625794734, 0.25703905540417804, 0.24069028156221617, 0.21707538601271573, 0.2688465031789282, 0.2615803814713896, 0.24341507720254316, 0.2761126248864669, 0.259763851044505, 0.26430517711171664, 0.25522252497729336, 0.254314259763851, 0.27157129881925524, 0.26612170753860126, 0.2579473206176203, 0.27974568574023617, 0.2742960944595822, 0.27520435967302453, 0.2815622161671208, 0.28701180744777477]

CBOW(
  (embed): Embedding(18280, 5)
  (linear): Linear(in_features=5, out_features=5, bias=True)
)
Iter 1000: loss=2839.5673, time=0.82s
iter 1000: dev acc=0.1871
new highscore
Iter 2000: loss=2094.7603, time=1.89s
iter 2000: dev acc=0.2016
new highscore
Iter 3000: loss=1685.2858, time=2.96s
iter 3000: dev acc=0.2298
new highscore
Iter 4000: loss=1623.3648, time=4.02s
iter 4000: dev acc=0.2289
Iter 5000: loss=1588.4818, time=5.08s
iter 5000: dev acc=0.2480
new highscore
Iter 6000: loss=1595.0155, time=6.14s
iter 6000: dev acc=0.2525
new highscore
Iter 7000: loss=1575.8724, time=7.20s
iter 7000: dev acc=0.2534
new highscore
Iter 8000: loss=1583.1896, time=8.25s
iter 8000: dev acc=0.2716
new highscore
Iter 9000: loss=1579.1932, time=9.30s
iter 9000: dev acc=0.2589
Iter 10000: loss=1577.9192, time=10.33s
iter 10000: dev acc=0.2607
Iter 11000: loss=1566.5233, time=11.37s
iter 11000: dev acc=0.2906
new highscore
Iter 12000: loss=1573.5811, time=12.41s
iter 12000: dev acc=0.2652
Iter 13000: loss=1568.3496, time=13.44s
iter 13000: dev acc=0.2688
Iter 14000: loss=1585.2525, time=14.48s
iter 14000: dev acc=0.2688
Iter 15000: loss=1580.9906, time=15.51s
iter 15000: dev acc=0.2616
Iter 16000: loss=1564.5799, time=16.54s
iter 16000: dev acc=0.2852
Iter 17000: loss=1574.7283, time=17.57s
iter 17000: dev acc=0.2716
Iter 18000: loss=1545.9639, time=18.61s
iter 18000: dev acc=0.2707
Iter 19000: loss=1566.2000, time=19.64s
iter 19000: dev acc=0.2698
Iter 20000: loss=1545.9902, time=20.67s
iter 20000: dev acc=0.2797
Iter 21000: loss=1540.1522, time=21.71s
iter 21000: dev acc=0.2979
new highscore
Iter 22000: loss=1526.7709, time=22.74s
iter 22000: dev acc=0.2616
Iter 23000: loss=1524.3336, time=23.78s
iter 23000: dev acc=0.2707
Iter 24000: loss=1538.8465, time=24.81s
iter 24000: dev acc=0.2870
Iter 25000: loss=1555.7141, time=25.84s
iter 25000: dev acc=0.2897
Iter 26000: loss=1526.3392, time=26.88s
iter 26000: dev acc=0.2952
Iter 27000: loss=1498.0003, time=27.91s
iter 27000: dev acc=0.2879
Iter 28000: loss=1485.9719, time=28.94s
iter 28000: dev acc=0.3115
new highscore
Iter 29000: loss=1498.5564, time=29.98s
iter 29000: dev acc=0.3070
Iter 30000: loss=1500.3161, time=31.01s
iter 30000: dev acc=0.3015
Done training
Loading best model
best model iter 28000: train acc=0.3395, dev acc=0.3115, test acc=0.2937
losses:
[2839.5673440932296, 2094.7603250406682, 1685.2857956066728, 1623.364757657051, 1588.4817699193954, 1595.015528023243, 1575.8723601698875, 1583.189639210701, 1579.1931960582733, 1577.919244170189, 1566.5233471393585, 1573.5810993909836, 1568.3495817780495, 1585.2524708509445, 1580.9906044006348, 1564.5799115896225, 1574.7282712459564, 1545.9639455676079, 1566.200000166893, 1545.990158110857, 1540.1522274017334, 1526.7709382772446, 1524.3335826396942, 1538.846458107233, 1555.7140574455261, 1526.3392366170883, 1498.0002831816673, 1485.9718871414661, 1498.5563945770264, 1500.3161067068577]
accuracies:
[0.18710263396911897, 0.2016348773841962, 0.22979109900090827, 0.22888283378746593, 0.24795640326975477, 0.2524977293369664, 0.25340599455040874, 0.27157129881925524, 0.25885558583106266, 0.26067211625794734, 0.29064486830154407, 0.2652134423251589, 0.2688465031789282, 0.2688465031789282, 0.2615803814713896, 0.2851952770208901, 0.27157129881925524, 0.2706630336058129, 0.26975476839237056, 0.27974568574023617, 0.29791099000908267, 0.2615803814713896, 0.2706630336058129, 0.28701180744777477, 0.28973660308810173, 0.29518619436875565, 0.28792007266121705, 0.3115349682107175, 0.3069936421435059, 0.30154405086285196]

CBOW(
  (embed): Embedding(18280, 5)
  (linear): Linear(in_features=5, out_features=5, bias=True)
)
Iter 1000: loss=4537.3085, time=0.82s
iter 1000: dev acc=0.2162
new highscore
Iter 2000: loss=2561.0023, time=1.88s
iter 2000: dev acc=0.2234
new highscore
Iter 3000: loss=2036.7236, time=2.95s
iter 3000: dev acc=0.2325
new highscore
Iter 4000: loss=1787.6957, time=4.01s
iter 4000: dev acc=0.2289
Iter 5000: loss=1675.7957, time=5.07s
iter 5000: dev acc=0.2389
new highscore
Iter 6000: loss=1608.3840, time=6.13s
iter 6000: dev acc=0.2525
new highscore
Iter 7000: loss=1612.2446, time=7.19s
iter 7000: dev acc=0.2434
Iter 8000: loss=1585.5649, time=8.23s
iter 8000: dev acc=0.2561
new highscore
Iter 9000: loss=1583.5816, time=9.28s
iter 9000: dev acc=0.2579
new highscore
Iter 10000: loss=1578.6024, time=10.31s
iter 10000: dev acc=0.2561
Iter 11000: loss=1569.1132, time=11.35s
iter 11000: dev acc=0.2788
new highscore
Iter 12000: loss=1563.0129, time=12.39s
iter 12000: dev acc=0.2779
Iter 13000: loss=1569.2627, time=13.42s
iter 13000: dev acc=0.2734
Iter 14000: loss=1562.4891, time=14.45s
iter 14000: dev acc=0.2534
Iter 15000: loss=1572.6608, time=15.49s
iter 15000: dev acc=0.2779
Iter 16000: loss=1562.4230, time=16.52s
iter 16000: dev acc=0.2725
Iter 17000: loss=1573.0573, time=17.56s
iter 17000: dev acc=0.2934
new highscore
Iter 18000: loss=1548.5274, time=18.60s
iter 18000: dev acc=0.2888
Iter 19000: loss=1536.5743, time=19.64s
iter 19000: dev acc=0.2916
Iter 20000: loss=1542.3465, time=20.68s
iter 20000: dev acc=0.2870
Iter 21000: loss=1556.1368, time=21.71s
iter 21000: dev acc=0.2698
Iter 22000: loss=1536.8322, time=22.75s
iter 22000: dev acc=0.2943
new highscore
Iter 23000: loss=1531.5562, time=23.79s
iter 23000: dev acc=0.2797
Iter 24000: loss=1543.6920, time=24.84s
iter 24000: dev acc=0.2725
Iter 25000: loss=1531.8525, time=25.88s
iter 25000: dev acc=0.2816
Iter 26000: loss=1532.3317, time=26.92s
iter 26000: dev acc=0.2707
Iter 27000: loss=1509.1770, time=27.96s
iter 27000: dev acc=0.2906
Iter 28000: loss=1526.0464, time=28.99s
iter 28000: dev acc=0.2834
Iter 29000: loss=1497.0222, time=30.03s
iter 29000: dev acc=0.2779
Iter 30000: loss=1495.9732, time=31.06s
iter 30000: dev acc=0.3079
new highscore
Done training
Loading best model
best model iter 30000: train acc=0.3462, dev acc=0.3079, test acc=0.3285
losses:
[4537.308537691332, 2561.0022749263735, 2036.723550375551, 1787.6956628523767, 1675.7956659793854, 1608.3839812874794, 1612.2445915341377, 1585.564901649952, 1583.5815894007683, 1578.6023604869843, 1569.113200187683, 1563.0129461288452, 1569.2627281546593, 1562.4890639185905, 1572.660835325718, 1562.4230206608772, 1573.0573230981827, 1548.527415752411, 1536.5742591321468, 1542.3464530706406, 1556.1368067860603, 1536.8322443962097, 1531.556243300438, 1543.692048072815, 1531.8525432944298, 1532.3316665291786, 1509.1770404577255, 1526.0463814735413, 1497.0222192704678, 1495.9732003211975]
accuracies:
[0.2161671207992734, 0.22343324250681199, 0.23251589464123523, 0.22888283378746593, 0.23887375113533152, 0.2524977293369664, 0.24341507720254316, 0.2561307901907357, 0.2579473206176203, 0.2561307901907357, 0.27883742052679383, 0.2779291553133515, 0.27338782924613986, 0.25340599455040874, 0.2779291553133515, 0.2724795640326976, 0.29336966394187103, 0.2888283378746594, 0.29155313351498635, 0.28701180744777477, 0.26975476839237056, 0.29427792915531337, 0.27974568574023617, 0.2724795640326976, 0.2815622161671208, 0.2706630336058129, 0.29064486830154407, 0.28337874659400547, 0.2779291553133515, 0.3079019073569482]

