CBOW(
  (embed): Embedding(18280, 5)
  (linear): Linear(in_features=5, out_features=5, bias=True)
)
Iter 1000: loss=3897.6167, time=0.88s
iter 1000: dev acc=0.2134
new highscore
Iter 2000: loss=2636.0955, time=1.95s
iter 2000: dev acc=0.2262
new highscore
Iter 3000: loss=2217.3798, time=3.01s
iter 3000: dev acc=0.2452
new highscore
Iter 4000: loss=1946.1817, time=4.07s
iter 4000: dev acc=0.2198
Iter 5000: loss=1831.0315, time=5.13s
iter 5000: dev acc=0.2634
new highscore
Iter 6000: loss=1750.9587, time=6.18s
iter 6000: dev acc=0.2543
Iter 7000: loss=1631.5246, time=7.23s
iter 7000: dev acc=0.2607
Iter 8000: loss=1613.7039, time=8.27s
iter 8000: dev acc=0.2543
Iter 9000: loss=1570.3317, time=9.31s
iter 9000: dev acc=0.2371
Iter 10000: loss=1591.4869, time=10.34s
iter 10000: dev acc=0.2570
Iter 11000: loss=1560.0535, time=11.37s
iter 11000: dev acc=0.2461
Iter 12000: loss=1586.3748, time=12.40s
iter 12000: dev acc=0.2570
Iter 13000: loss=1582.0358, time=13.43s
iter 13000: dev acc=0.2307
Iter 14000: loss=1571.3362, time=14.46s
iter 14000: dev acc=0.2589
Iter 15000: loss=1584.6138, time=15.49s
iter 15000: dev acc=0.2480
Iter 16000: loss=1584.3351, time=16.51s
iter 16000: dev acc=0.2607
Iter 17000: loss=1572.1174, time=17.54s
iter 17000: dev acc=0.2598
Iter 18000: loss=1568.2058, time=18.57s
iter 18000: dev acc=0.2307
Iter 19000: loss=1556.3070, time=19.60s
iter 19000: dev acc=0.2525
Iter 20000: loss=1552.4266, time=20.63s
iter 20000: dev acc=0.2534
Iter 21000: loss=1577.5143, time=21.66s
iter 21000: dev acc=0.2352
Iter 22000: loss=1565.0604, time=22.69s
iter 22000: dev acc=0.2507
Iter 23000: loss=1547.9414, time=23.72s
iter 23000: dev acc=0.2407
Iter 24000: loss=1558.7348, time=24.75s
iter 24000: dev acc=0.2670
new highscore
Iter 25000: loss=1555.8570, time=25.79s
iter 25000: dev acc=0.2407
Iter 26000: loss=1550.3591, time=26.83s
iter 26000: dev acc=0.2734
new highscore
Iter 27000: loss=1552.3936, time=27.86s
iter 27000: dev acc=0.2461
Iter 28000: loss=1536.8776, time=28.89s
iter 28000: dev acc=0.2398
Iter 29000: loss=1543.9186, time=29.92s
iter 29000: dev acc=0.2725
Iter 30000: loss=1549.4807, time=30.94s
iter 30000: dev acc=0.2525
Done training
Loading best model
best model iter 26000: train acc=0.3001, dev acc=0.2734, test acc=0.2588
losses:
[3897.616731618182, 2636.0955221927725, 2217.37981069088, 1946.1817167848349, 1831.0314634144306, 1750.9586641788483, 1631.5246396660805, 1613.7039272785187, 1570.3317387104034, 1591.4868532419205, 1560.0534555315971, 1586.374758720398, 1582.0358237028122, 1571.3362092971802, 1584.6137927174568, 1584.335065484047, 1572.1174350380898, 1568.2057573795319, 1556.3069906830788, 1552.4265842437744, 1577.5143430829048, 1565.0604038238525, 1547.9413505196571, 1558.7347725629807, 1555.8569756150246, 1550.359070688486, 1552.3935719132423, 1536.8775764107704, 1543.918605774641, 1549.480697274208]
accuracies:
[0.2134423251589464, 0.22615803814713897, 0.2452316076294278, 0.2198001816530427, 0.2633969118982743, 0.254314259763851, 0.26067211625794734, 0.254314259763851, 0.23705722070844687, 0.25703905540417804, 0.24613987284287012, 0.25703905540417804, 0.23069936421435058, 0.25885558583106266, 0.24795640326975477, 0.26067211625794734, 0.259763851044505, 0.23069936421435058, 0.2524977293369664, 0.25340599455040874, 0.23524069028156222, 0.2506811989100817, 0.24069028156221617, 0.2670299727520436, 0.24069028156221617, 0.27338782924613986, 0.24613987284287012, 0.23978201634877383, 0.2724795640326976, 0.2524977293369664]

CBOW(
  (embed): Embedding(18280, 5)
  (linear): Linear(in_features=5, out_features=5, bias=True)
)
Iter 1000: loss=2748.2329, time=0.82s
iter 1000: dev acc=0.1889
new highscore
Iter 2000: loss=2043.4869, time=1.89s
iter 2000: dev acc=0.2071
new highscore
Iter 3000: loss=1771.2721, time=2.95s
iter 3000: dev acc=0.2053
Iter 4000: loss=1623.1773, time=4.01s
iter 4000: dev acc=0.2407
new highscore
Iter 5000: loss=1587.6260, time=5.07s
iter 5000: dev acc=0.2570
new highscore
Iter 6000: loss=1575.1919, time=6.13s
iter 6000: dev acc=0.2779
new highscore
Iter 7000: loss=1583.9485, time=7.18s
iter 7000: dev acc=0.2616
Iter 8000: loss=1584.9485, time=8.20s
iter 8000: dev acc=0.2543
Iter 9000: loss=1589.2747, time=9.22s
iter 9000: dev acc=0.2416
Iter 10000: loss=1582.8610, time=10.23s
iter 10000: dev acc=0.2289
Iter 11000: loss=1569.5233, time=11.24s
iter 11000: dev acc=0.2552
Iter 12000: loss=1566.5423, time=12.25s
iter 12000: dev acc=0.2734
Iter 13000: loss=1565.7716, time=13.26s
iter 13000: dev acc=0.2816
new highscore
Iter 14000: loss=1582.1599, time=14.28s
iter 14000: dev acc=0.2570
Iter 15000: loss=1587.9455, time=15.28s
iter 15000: dev acc=0.2634
Iter 16000: loss=1554.8586, time=16.30s
iter 16000: dev acc=0.2825
new highscore
Iter 17000: loss=1563.4704, time=17.31s
iter 17000: dev acc=0.2825
Iter 18000: loss=1558.8696, time=18.32s
iter 18000: dev acc=0.2761
Iter 19000: loss=1545.9441, time=19.33s
iter 19000: dev acc=0.2825
Iter 20000: loss=1544.1989, time=20.34s
iter 20000: dev acc=0.2534
Iter 21000: loss=1552.8490, time=21.35s
iter 21000: dev acc=0.2834
new highscore
Iter 22000: loss=1528.1561, time=22.36s
iter 22000: dev acc=0.3034
new highscore
Iter 23000: loss=1544.1948, time=23.37s
iter 23000: dev acc=0.2897
Iter 24000: loss=1551.1708, time=24.40s
iter 24000: dev acc=0.2961
Iter 25000: loss=1508.4418, time=25.43s
iter 25000: dev acc=0.2943
Iter 26000: loss=1512.5268, time=26.46s
iter 26000: dev acc=0.2952
Iter 27000: loss=1486.0611, time=27.49s
iter 27000: dev acc=0.3015
Iter 28000: loss=1504.6953, time=28.52s
iter 28000: dev acc=0.3034
Iter 29000: loss=1496.0917, time=29.55s
iter 29000: dev acc=0.2906
Iter 30000: loss=1523.6631, time=30.59s
iter 30000: dev acc=0.3079
new highscore
Done training
Loading best model
best model iter 30000: train acc=0.3470, dev acc=0.3079, test acc=0.3014
losses:
[2748.2329265167937, 2043.4868803322315, 1771.272121295333, 1623.1772608160973, 1587.6260227560997, 1575.1919190883636, 1583.9484981894493, 1584.948499917984, 1589.2746886014938, 1582.8609994649887, 1569.5232745409012, 1566.542340874672, 1565.7715657353401, 1582.1598525643349, 1587.945508658886, 1554.858614563942, 1563.4704488515854, 1558.8696085214615, 1545.9440667033195, 1544.1988590955734, 1552.848953485489, 1528.1561316251755, 1544.1947563886642, 1551.1708129644394, 1508.4417923688889, 1512.5267706513405, 1486.0611221790314, 1504.695321381092, 1496.091726243496, 1523.6631398200989]
accuracies:
[0.18891916439600362, 0.20708446866485014, 0.2052679382379655, 0.24069028156221617, 0.25703905540417804, 0.2779291553133515, 0.2615803814713896, 0.254314259763851, 0.24159854677565848, 0.22888283378746593, 0.25522252497729336, 0.27338782924613986, 0.2815622161671208, 0.25703905540417804, 0.2633969118982743, 0.28247048138056313, 0.28247048138056313, 0.2761126248864669, 0.28247048138056313, 0.25340599455040874, 0.28337874659400547, 0.3033605812897366, 0.28973660308810173, 0.296094459582198, 0.29427792915531337, 0.29518619436875565, 0.30154405086285196, 0.3033605812897366, 0.29064486830154407, 0.3079019073569482]

CBOW(
  (embed): Embedding(18280, 5)
  (linear): Linear(in_features=5, out_features=5, bias=True)
)
Iter 1000: loss=4748.9035, time=0.82s
iter 1000: dev acc=0.2216
new highscore
Iter 2000: loss=2489.8080, time=1.89s
iter 2000: dev acc=0.2361
new highscore
Iter 3000: loss=2043.2914, time=2.95s
iter 3000: dev acc=0.2307
Iter 4000: loss=1774.3432, time=4.01s
iter 4000: dev acc=0.2461
new highscore
Iter 5000: loss=1672.5321, time=5.07s
iter 5000: dev acc=0.2280
Iter 6000: loss=1619.5694, time=6.12s
iter 6000: dev acc=0.2516
new highscore
Iter 7000: loss=1605.3677, time=7.18s
iter 7000: dev acc=0.2543
new highscore
Iter 8000: loss=1577.8533, time=8.22s
iter 8000: dev acc=0.2561
new highscore
Iter 9000: loss=1596.2563, time=9.26s
iter 9000: dev acc=0.2625
new highscore
Iter 10000: loss=1586.9048, time=10.30s
iter 10000: dev acc=0.2643
new highscore
Iter 11000: loss=1573.6000, time=11.34s
iter 11000: dev acc=0.2579
Iter 12000: loss=1570.9796, time=12.37s
iter 12000: dev acc=0.2607
Iter 13000: loss=1552.2833, time=13.40s
iter 13000: dev acc=0.2661
new highscore
Iter 14000: loss=1586.1600, time=14.44s
iter 14000: dev acc=0.2607
Iter 15000: loss=1554.2241, time=15.47s
iter 15000: dev acc=0.2670
new highscore
Iter 16000: loss=1552.2854, time=16.51s
iter 16000: dev acc=0.2852
new highscore
Iter 17000: loss=1568.5814, time=17.54s
iter 17000: dev acc=0.2925
new highscore
Iter 18000: loss=1552.2186, time=18.58s
iter 18000: dev acc=0.2861
Iter 19000: loss=1569.1896, time=19.59s
iter 19000: dev acc=0.2797
Iter 20000: loss=1532.2763, time=20.60s
iter 20000: dev acc=0.2870
Iter 21000: loss=1547.6837, time=21.61s
iter 21000: dev acc=0.2807
Iter 22000: loss=1546.0401, time=22.62s
iter 22000: dev acc=0.2925
Iter 23000: loss=1540.9488, time=23.63s
iter 23000: dev acc=0.2816
Iter 24000: loss=1542.5136, time=24.64s
iter 24000: dev acc=0.2916
Iter 25000: loss=1520.2042, time=25.65s
iter 25000: dev acc=0.2834
Iter 26000: loss=1517.1475, time=26.67s
iter 26000: dev acc=0.3088
new highscore
Iter 27000: loss=1527.0722, time=27.68s
iter 27000: dev acc=0.2916
Iter 28000: loss=1504.3351, time=28.69s
iter 28000: dev acc=0.2934
Iter 29000: loss=1524.1091, time=29.71s
iter 29000: dev acc=0.3015
Iter 30000: loss=1517.7321, time=30.73s
iter 30000: dev acc=0.2979
Done training
Loading best model
best model iter 26000: train acc=0.3352, dev acc=0.3088, test acc=0.3149
losses:
[4748.9035356867325, 2489.8079701349197, 2043.2914266586304, 1774.3432015776634, 1672.5321361720562, 1619.5694133341312, 1605.3676949739456, 1577.8532916009426, 1596.2562528848648, 1586.9048473834991, 1573.6000248789787, 1570.9796465039253, 1552.283310174942, 1586.160043656826, 1554.2241208553314, 1552.2854114472866, 1568.5814051628113, 1552.2185769677162, 1569.189630806446, 1532.2762605845928, 1547.683700233698, 1546.0401304662228, 1540.9487929344177, 1542.5136011838913, 1520.204209625721, 1517.147540152073, 1527.0722180008888, 1504.335149884224, 1524.1091313362122, 1517.7320672273636]
accuracies:
[0.22161671207992734, 0.23614895549500453, 0.23069936421435058, 0.24613987284287012, 0.22797456857402362, 0.25158946412352406, 0.254314259763851, 0.2561307901907357, 0.26248864668483196, 0.26430517711171664, 0.2579473206176203, 0.26067211625794734, 0.26612170753860126, 0.26067211625794734, 0.2670299727520436, 0.2851952770208901, 0.2924613987284287, 0.28610354223433243, 0.27974568574023617, 0.28701180744777477, 0.28065395095367845, 0.2924613987284287, 0.2815622161671208, 0.29155313351498635, 0.28337874659400547, 0.30881017257039056, 0.29155313351498635, 0.29336966394187103, 0.30154405086285196, 0.29791099000908267]

